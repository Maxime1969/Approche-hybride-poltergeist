{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CODE VECTORIZATION"
      ],
      "metadata": {
        "id": "RJbQzR-tVxcC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kldnnVHAKHPA"
      },
      "outputs": [],
      "source": [
        "\n",
        "#import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class ClsEmbedding():\n",
        "   \"\"\"\n",
        "    Creates an ClsEmbedding object. This object transforms all the code snippets into a dataframe.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    data : dataframe\n",
        "        dataframe is obtained from the csv file containing the file names. the path of the csv file is indicated.\n",
        "    datafile: string\n",
        "       the path to the folder containing the files.\n",
        "    tokenizer: tokenizer\n",
        "       it is obtained from RobertaTokenizer.from_pretrained(model_name)\n",
        "    model: T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "       pre-trained model\n",
        "    batch: int\n",
        "        the number of samples per batch.\n",
        "    Methods\n",
        "    -------\n",
        "    get_embedding()\n",
        "        transforms all the code snippets into a dataframe\n",
        "    get_matrix()\n",
        "        obtain the matrix of embeddings\n",
        "    \"\"\"\n",
        "   def __init__(self, data, datafile, tokenizer, model, batch):\n",
        "\n",
        "     self.data = data\n",
        "     self.datafile = datafile\n",
        "\n",
        "     self.tokenizer = tokenizer\n",
        "     self.model = model.to(device)\n",
        "     self.stepbatch = 0\n",
        "     self.batch = batch\n",
        "     self.matrix_data= pd.DataFrame(columns=['Matrix'])\n",
        "\n",
        "   #Embedding\n",
        "   def get_embedding(self):\n",
        "\n",
        "     X_data = self.data.iloc[:, 0].tolist()\n",
        "     ID_data = self.data.index.tolist()\n",
        "     if len(X_data)%self.batch == 0:\n",
        "       self.stepbatch = int((len(X_data)/self.batch))\n",
        "     else:\n",
        "       self.stepbatch = int((len(X_data)//self.batch)) + 1\n",
        "     for i in range(int(self.stepbatch)):\n",
        "      batch_samples =[]\n",
        "      generator_ = []\n",
        "      generator_index = []\n",
        "      if len(X_data) - i*(self.batch + 1) <= self.batch:\n",
        "        batch_samples = X_data[i*(self.batch + 1): len(X_data)]\n",
        "        batch_ID = ID_data[i*(self.batch + 1): len(X_data)]\n",
        "      else:\n",
        "        batch_samples = X_data[i*(self.batch + 1):i*(self.batch + 1) + self.batch]\n",
        "        batch_ID = ID_data[i*(self.batch + 1):i*(self.batch + 1) + self.batch]\n",
        "\n",
        "      for k, batchs in enumerate(batch_samples):  # Loop with step of 'batch'\n",
        "        print(self.datafile + \"/\" + batchs)\n",
        "        if os.path.exists(self.datafile + \"/\" + batchs):\n",
        "          with open(self.datafile + \"/\" + batchs, 'rb') as file:\n",
        "            detected_encoding = chardet.detect(file.read())['encoding']\n",
        "          with open(self.datafile + \"/\" + batchs, \"r\", encoding= detected_encoding) as fichier:\n",
        "            contenu = fichier.readlines()\n",
        "            if contenu!=[]:\n",
        "              inputs = self.tokenizer.encode_plus(contenu, padding='longest', truncation=True, return_tensors='pt')\n",
        "              inputs = inputs.to(device)\n",
        "              outputs = self.model(inputs.input_ids, attention_mask=inputs.attention_mask, decoder_input_ids = inputs['input_ids'], output_hidden_states=True )\n",
        "              embedding = outputs.encoder_last_hidden_state\n",
        "              embedding = embedding.to(device)\n",
        "              mean = pt.mean(embedding, dim=(1,2))\n",
        "              std = pt.std(embedding, dim=(1,2))\n",
        "              normalized_embedding = (embedding - mean) / std\n",
        "              normalized_embedding = normalized_embedding.to(device)\n",
        "              reduced_normalized_embedding = pt.mean(normalized_embedding, dim=0).to(device)\n",
        "              fused_normalized_embeddings = pt.mean(reduced_normalized_embedding, dim=0).to(device)\n",
        "              x_normalized = (fused_normalized_embeddings - fused_normalized_embeddings.min()) / (fused_normalized_embeddings.max() - fused_normalized_embeddings.min())\n",
        "              generator_.append(x_normalized.cpu().detach().numpy())\n",
        "              generator_index.append(batch_ID[k])\n",
        "        else:\n",
        "          print(\"Fichier introuvable\")\n",
        "\n",
        "      if len(generator_index) == len(generator_):\n",
        "        df = pd.DataFrame({'Identity': generator_index, 'Matrix': generator_})\n",
        "        yield df\n",
        "\n",
        "   def get_matrix(self):\n",
        "    matrix_data = []\n",
        "    matrix_index = []\n",
        "    generator = self.get_embedding()\n",
        "    for j, data_X in enumerate(generator):\n",
        "      if j == 0:\n",
        "        matrix_data = data_X.iloc[:, 1:]\n",
        "        matrix_index = data_X.iloc[:, 0]\n",
        "      else:\n",
        "        if j > 0:\n",
        "          matrix_data = pd.concat([matrix_data, data_X.iloc[:, 1:]], axis = 0)\n",
        "          matrix_index = pd.concat([matrix_index, data_X.iloc[:, 0]], axis = 0)\n",
        "    flattened_data = matrix_data.iloc[:, 0].apply(lambda x: x.flatten())\n",
        "    new_matrix_data = pd.DataFrame(flattened_data.tolist(), index=matrix_index.tolist())\n",
        "    return new_matrix_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING DATA AND EMBEDDING MODEL"
      ],
      "metadata": {
        "id": "HjdADKdEYy_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as pt\n",
        "import gc\n",
        "import chardet\n",
        "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
        "import requests\n",
        "import pandas as pd\n",
        "#Setting device\n",
        "if pt.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "  pt.cuda.empty_cache()\n",
        "  # Enable GPU.\n",
        "  pt.cuda.set_device(0)\n",
        "else:\n",
        "  device =\"cpu\"\n",
        "gc.collect()\n",
        "#Parameter\n",
        "batch = 100\n",
        "#embedding model\n",
        "model_name = 'Salesforce/codet5-small'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def get_load_test (file_csv, file_java):\n",
        "  # Download CSV\n",
        "  url = \"https://raw.githubusercontent.com/Maxime1969/Approche-hybride-poltergeist/main/\" + file_csv\n",
        "  response = requests.get(url)\n",
        "  with open(file_csv, 'wb') as file:\n",
        "    file.write(response.content)\n",
        "  #loading data\n",
        "  data_ = pd.read_csv(file_csv, sep =',', index_col = \"ID\")\n",
        "  data_ = resample(data_, replace = False, n_samples = len(data_), random_state=42)\n",
        "  datafile = \"https://raw.githubusercontent.com/Maxime1969/Approche-hybride-poltergeist/main/\" + file_java\n",
        "  embedding = ClsEmbedding(data_, datafile, tokenizer, model, batch)\n",
        "  X_data = embedding.get_matrix()\n",
        "  return X_data"
      ],
      "metadata": {
        "id": "kllFMI88Wb2Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "class CodeDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        if not isinstance(dataframe, pd.DataFrame):\n",
        "            raise TypeError(\"Input 'dataframe' must be a pandas DataFrame.\")\n",
        "        self.data = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        code = self.data.iloc[idx].values\n",
        "        return code"
      ],
      "metadata": {
        "id": "S4ZON1f2RQUB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHECK THE PARTITIONING OF DATA SET INTO TWO SUBSETS"
      ],
      "metadata": {
        "id": "oCVhedN1eZso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "X_data = get_load_test(\"apache-ant.csv\", \"File_apache-ant-1.6.2\")\n",
        "inert = []\n",
        "for i in range(1, 10):\n",
        "   kmeans = KMeans(n_clusters=i, random_state=42, n_init= 10)\n",
        "   # Train Model\n",
        "   kmeans.fit(X_data)\n",
        "   inert.append(kmeans.inertia_)\n",
        "\n",
        "#Elbow method\n",
        "plt.plot(range(1,10), inert)\n",
        "plt.title(\"Elbow Method\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BbfUIHZed6Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ISOLATION MODEL"
      ],
      "metadata": {
        "id": "fPE0wItbOt-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "alloutliers = []\n",
        "\n",
        "# definir le modele IsolationForest\n",
        "Seuil = [0.04, 0.05, 0.07, 0.08, 0.1, 0.2]\n",
        "\n",
        "for seuil in Seuil:\n",
        "  listoutliers = []\n",
        "  for i in range(100):\n",
        "    model = IsolationForest(n_estimators= 100, max_samples=256, contamination = seuil, max_features = X_data.shape[1])\n",
        "    model.fit(X_data)\n",
        "    X_data['anomaly']= model.predict(X_data)\n",
        "    outliers = [myindex for myindex in X_data.index if X_data.loc[myindex, 'anomaly'] == -1]\n",
        "    listoutliers.extend(outliers)\n",
        "    X_data= X_data.drop(columns=X_data.columns[-1])\n",
        "  alloutliers.append(listoutliers)\n",
        "\n",
        "ens_outliers = {item for sublist in alloutliers for item in sublist}\n",
        "\n",
        "list_outliers = list(ens_outliers)\n",
        "X_outliers = X_data.loc[list_outliers]\n",
        "print(\"number of anomalies suggested by Isolation Forest\", len(list_outliers))"
      ],
      "metadata": {
        "id": "Tg2DC0iUbjAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINITION OF THE VAE MODEL"
      ],
      "metadata": {
        "id": "jK2zinK2OUNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Définir l'architecture du VAE\n",
        "class ClsVAE(nn.Module):\n",
        "    def __init__(self, input_dim, hiden_dim, latent_dim):\n",
        "        super(ClsVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hiden_dim),\n",
        "            #nn.ReLU(),\n",
        "            nn.LeakyReLU(0.3),\n",
        "            nn.Linear(hiden_dim, hiden_dim // 2),\n",
        "            #nn.ReLU(),\n",
        "            nn.LeakyReLU(0.3),\n",
        "            nn.Linear(hiden_dim // 2, latent_dim * 2)  # Le double de latent_dim pour les moyennes et les log_variances\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hiden_dim // 2),\n",
        "            #nn.ReLU(),\n",
        "            nn.LeakyReLU(0.3),\n",
        "            nn.Linear(hiden_dim//2, hiden_dim),\n",
        "            #nn.ReLU(),\n",
        "            nn.LeakyReLU(0.3),\n",
        "            nn.Linear(hiden_dim, input_dim),\n",
        "            nn.Sigmoid()  # Pour s'assurer que les valeurs reconstruites sont comprises entre 0 et 1\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = pt.exp(0.5 * log_var)\n",
        "        eps = pt.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_output = self.encoder(x)\n",
        "        mu, log_var = enc_output[:, :self.latent_dim], enc_output[:, self.latent_dim:]\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        dec_output = self.decoder(z)\n",
        "        return dec_output, mu, log_var, z\n",
        "\n",
        "# Définir la fonction perte ELBO (Evidence Lower Bound)\n",
        "def loss_function(recon_x, x, mu, log_var, beta):\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x.float(), x.float(), reduction='sum')\n",
        "    KLD = -0.5 * pt.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + beta*KLD\n",
        "\n",
        "# Entraînement du VAE\n",
        "def train_vae(vae, train_dataloader, optimizer, num_epochs, test_dataloader, beta):\n",
        "    vae.train()\n",
        "    losses_tain = []\n",
        "    #losses_test = []\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for inputs in train_dataloader:\n",
        "            print(inputs.shape)\n",
        "            optimizer.zero_grad()\n",
        "            recon_batch, mu, log_var, z = vae(inputs.float())\n",
        "            loss = loss_function(recon_batch.float(), inputs.float(), mu, log_var, beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= len(train_dataloader.dataset)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "        losses_tain.append(epoch_loss)\n",
        "\n",
        "        # Evaluate on test data\n",
        "        test_loss = evaluate_vae(vae, test_dataloader, beta)\n",
        "        #losses_test.append(test_loss)\n",
        "    return losses_tain\n",
        "\n",
        "def evaluate_vae(vae, test_dataloader, beta):\n",
        "    vae.eval()\n",
        "    total_loss = 0.0\n",
        "    with pt.no_grad():\n",
        "       for inputs in test_dataloader:\n",
        "            recon_batch, mu, log_var, z = vae(inputs.float())\n",
        "            loss = loss_function(recon_batch.float(), inputs.float(), mu, log_var, beta)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(test_dataloader.dataset)\n",
        "\n",
        "def min_max_scaling(original_list):\n",
        "    # Find the minimum and maximum values in the original list\n",
        "    min_value = min(original_list)\n",
        "    max_value = max(original_list)\n",
        "\n",
        "    # Create a new empty list to store the normalized values\n",
        "    normalized_list = []\n",
        "\n",
        "    # Normalize each item in the original list\n",
        "    for item in original_list:\n",
        "        normalized_value = (item - min_value) / (max_value - min_value)\n",
        "        normalized_list.append(normalized_value)\n",
        "\n",
        "    return normalized_list\n",
        "# Anomaly detection using VAE\n",
        "def anomaly_detection(vae, X_anomalous, threshold, numbersample):\n",
        "    vae.eval()\n",
        "    with pt.no_grad():\n",
        "      anomaly_indices = []\n",
        "      reconstruction_prob_sum = 0\n",
        "      for x in X_anomalous:\n",
        "         reconstruction_prob_sum = 0\n",
        "         _, mu_z, log_var_z, _ = vae(x.float())  # Forward pass through the encoder\n",
        "         for _ in range(numbersample):\n",
        "           z = vae.reparameterize(mu_z, log_var_z)\n",
        "           recon_x = vae.decoder(z)  # Forward pass through the decoder\n",
        "           reconstruction_prob = F.binary_cross_entropy(recon_x.float(), x.float(), reduction='sum')  # Calculate reconstruction probability\n",
        "           reconstruction_prob_sum += reconstruction_prob.item()\n",
        "         reconstruction_prob_avg = reconstruction_prob_sum / numbersample\n",
        "         # Flag anomaly based on threshold\n",
        "         #if reconstruction_prob_avg < threshold:\n",
        "         anomaly_indices.append(reconstruction_prob_avg)\n",
        "    normalized_list = min_max_scaling(anomaly_indices)\n",
        "    return normalized_list\n",
        "\n",
        "def plot_loss_curves(train_losses):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(epochs, train_losses, label='Fonction Perte')\n",
        "    #plt.plot(epochs, test_losses, label='Test Loss', marker='s')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Perte')\n",
        "    plt.title(' Fonction perte')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Fonction pour calculer la probabilité de reconstruction\n",
        "def reconstruction_probability(x, encoder, decoder):\n",
        "    mean, log_var = encoder(x)\n",
        "\n",
        "    # Échantillonnage de z à partir de la distribution normale\n",
        "    epsilon = pt.randn_like(mean)\n",
        "    z = mean + pt.exp(0.5 * log_var) * epsilon\n",
        "\n",
        "    # Reconstruction de x à partir de z\n",
        "    reconstructed_x = decoder(z)\n",
        "\n",
        "    # Calcul de la log-vraisemblance marginale\n",
        "    log_likelihood = F.binary_cross_entropy(reconstructed_x, x, reduction='none').sum(dim=1)\n",
        "\n",
        "    # Calcul de la probabilité de reconstruction\n",
        "    reconstruction_prob = -0.5 * pt.sum(log_var + log_likelihood)\n",
        "\n",
        "    return reconstruction_prob"
      ],
      "metadata": {
        "id": "LZT6K2PSbi9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "#Parameters\n",
        "batch_size = 100\n",
        "hidden_dim = 500\n",
        "latent_dim = 250  # Latent space dimension\n",
        "lr = 1e-3\n",
        "epochs = 100\n",
        "beta = 4.0  # Parameter β\n",
        "\n",
        "data_train, data_test = train_test_split(X_data, test_size=0.20, random_state=42)\n",
        "datasetrain = CodeDataset(data_train)\n",
        "X_dataloader_train = DataLoader(datasetrain, batch_size=batch_size, shuffle=True)\n",
        "datasetest = CodeDataset(data_test)\n",
        "X_dataloader_test = DataLoader(datasetest, batch_size=batch_size, shuffle=True)\n",
        "# Initialiser le modèle et l'optimiseur\n",
        "vae = ClsVAE(X_data.shape[1], hidden_dim, latent_dim)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
        "\n",
        "# train VAE\n",
        "train_losses = vae.train_vae(vae, X_dataloader_train, optimizer, epochs, X_dataloader_test, beta)\n",
        "\n",
        "#Loading true positive data\n",
        "matrixpoltergeist = get_load_test(\"poltergeists.csv\", \"File_Poltergeists\")\n",
        "datasetvalue = CodeDataset(matrixpoltergeist)\n",
        "X_dataloader_evalue = DataLoader(datasetvalue, batch_size=1, shuffle=True)\n",
        "\n",
        "#Loading true negative data\n",
        "matrixfaux  = get_load_test(\"FalsePositive.csv\", \"File_FalsePositifs\")\n",
        "datasetvaluefaux = CodeDataset(matrixfaux)\n",
        "X_dataloaderfaux_evalue = DataLoader(datasetvaluefaux, batch_size=1, shuffle=True)\n",
        "\n",
        "#detection validation\n",
        "threshold = [0.05, 0.06, 0.07, 0.08, 0.1]\n",
        "numbersample = 10\n",
        "for seuil in threshold:\n",
        "  anomaly_indices = vae.anomaly_detection(vae, X_anomalous = X_dataloader_evalue, threshold = seuil, numbersample = numbersample)\n",
        "  print(anomaly_indices)\n",
        "  anomaly_faux = vae.anomaly_detection(vae, X_anomalous = X_dataloaderfaux_evalue, threshold = seuil, numbersample = numbersample)\n",
        "  print(anomaly_faux)\n",
        "  sampletrue = range(1, len( X_dataloader_evalue.dataset) + 1)\n",
        "  samplefalse = range(1, len( X_dataloaderfaux_evalue.dataset) + 1)\n",
        "  sample = range(1, numbersample + 2)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.plot(sampletrue, anomaly_indices, label='Probability of reconstruction of true positives',  marker='o', color='black')\n",
        "  plt.plot(samplefalse, anomaly_faux, label='Probability of reconstruction of false positives', marker='o', color='blue')\n",
        "  plt.plot(samplefalse, [seuil for _ in samplefalse]  , label='Reconstruction Probability Threshold', color='red')\n",
        "  plt.xlabel('sample number')\n",
        "  plt.ylabel('Probability of reconstruction')\n",
        "  plt.title(' Probability of reconstruction')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "mGAGCHtAO8mY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}